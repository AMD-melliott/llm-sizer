models:
- hf_model_id: meta-llama/Llama-2-7b-chat-hf
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 4
  - 8
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-validator
  input_lengths:
  - 512
  notes: Baseline small model
- hf_model_id: mistralai/Mistral-7B-Instruct-v0.3
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 4
  - 8
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-validator
  input_lengths:
  - 512
  notes: Popular instruction-tuned model
- hf_model_id: meta-llama/Llama-2-13b-chat-hf
  tensor_parallel_size: 2
  batch_sizes:
  - 1
  - 4
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-validator
  input_lengths:
  - 512
  notes: Medium model with 2-way TP
- hf_model_id: Qwen/Qwen2.5-32B-Instruct
  tensor_parallel_size: 2
  batch_sizes:
  - 1
  - 4
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-validator
  input_lengths:
  - 512
  notes: Qwen 2.5 architecture
- hf_model_id: zai-org/GLM-4.5-Air
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  - 8
  output_lengths:
  - 256
  - 512
  dtype: float16
  container_name: vllm-validator
  input_lengths:
  - 256
  - 512
  notes: MoE model - already profiled, include for completeness
- hf_model_id: 01-ai/Yi-34B-Chat
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-validator
  input_lengths:
  - 512
  notes: Yi architecture baseline
- hf_model_id: zai-org/GLM-4.5
  tensor_parallel_size: 8
  batch_sizes:
  - 1
  - 4
  - 8
  output_lengths:
  - 256
  - 512
  dtype: float16
  container_name: vllm-validator
  input_lengths:
  - 256
  - 512
  notes: MoE model - already profiled, include for completeness
- hf_model_id: meta-llama/Llama-2-70b-chat-hf
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 2
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-validator
  input_lengths:
  - 512
  notes: Large dense model - 4-way TP
- hf_model_id: Qwen/Qwen2.5-72B-Instruct
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 2
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-validator
  input_lengths:
  - 512
  notes: Large Qwen model
- hf_model_id: meta-llama/Llama-2-7b-chat-hf
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 4
  quantization: fp8
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-validator
  input_lengths:
  - 512
  notes: FP8 quantization test
- hf_model_id: mistralai/Mistral-7B-Instruct-v0.3
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 4
  quantization: int8
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-validator
  input_lengths:
  - 512
  notes: INT8 quantization test
- hf_model_id: meta-llama/Llama-2-13b-chat-hf
  tensor_parallel_size: 2
  batch_sizes:
  - 1
  output_lengths:
  - 128
  - 256
  - 1024
  - 2048
  dtype: float16
  container_name: vllm-validator
  input_lengths:
  - 128
  - 256
  - 1024
  - 2048
  notes: Sequence length scaling test
- hf_model_id: meta-llama/Llama-2-13b-chat-hf
  tensor_parallel_size: 2
  batch_sizes:
  - 1
  - 2
  - 4
  - 8
  - 16
  - 32
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-validator
  input_lengths:
  - 512
  notes: Batch size scaling test
- hf_model_id: deepseek-ai/deepseek-coder-33b-instruct
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-validator
  input_lengths:
  - 512
  notes: DeepSeek architecture
- hf_model_id: codellama/CodeLlama-34b-Instruct-hf
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-validator
  input_lengths:
  - 512
  notes: Code-specialized Llama
