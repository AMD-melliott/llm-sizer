models:
- hf_model_id: amd/Llama-3.2-1B-Instruct-FP8-KV
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 2
  - 4
  - 8
  - 16
  - 32
  - 64
  - 128
  quantization: null
  output_lengths:
  - 512
  kv_cache_dtype: fp8
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 1024
- hf_model_id: mistralai/Mistral-7B-Instruct-v0.2
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 2
  - 4
  - 8
  - 16
  - 32
  - 64
  quantization: null
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 1024
- hf_model_id: meta-llama/Llama-2-13b-hf
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 2
  - 4
  - 8
  - 16
  - 32
  quantization: null
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 1024
- hf_model_id: deepseek-ai/DeepSeek-R1-0528
  tensor_parallel_size: 2
  batch_sizes:
  - 1
  - 2
  - 4
  - 8
  - 16
  quantization: null
  output_lengths:
  - 512
  dtype: bfloat16
  trust_remote_code: true
  container_name: vllm-inference
  input_lengths:
  - 1024
