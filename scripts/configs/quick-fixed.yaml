# Quick Test Configuration (Enhanced v2.0) - WITH FIXES
# 
# Fixed version with trust_remote_code flag for Kimi-K2
#
# Usage:
#   python scripts/batch-profile-bench-enhanced.py --config scripts/configs/quick-fixed.yaml
#
# Estimated time: 2-3 hours

models:
  # GLM-4.5-Air (MoE model, medium complexity)
  - model_id: "zai-org/GLM-4.5-Air"
    container_name: "vllm-inference"
    input_lengths: [512]
    output_lengths: [512]
    batch_sizes: [1, 8]
    dtype: "bfloat16"
    tensor_parallel_size: 4
    quantization: null
    trust_remote_code: false
    model_params: 54.0e9
    num_layers: 40
    num_heads: 40
    hidden_size: 6144
    head_dim: 96
  
  # Llama-3.2-1B (smallest, fastest to test)
  - model_id: "amd/Llama-3.2-1B-Instruct-FP8-KV"
    container_name: "vllm-inference"
    input_lengths: [512, 1024]
    output_lengths: [512]
    batch_sizes: [1, 8, 16]
    dtype: "float16"
    tensor_parallel_size: 1
    quantization: null
    trust_remote_code: false
    model_params: 1.24e9
    num_layers: 16
    num_heads: 32
    hidden_size: 2048
    head_dim: 64
    kv_cache_dtype: "fp8"
  
  # Llama-3.1-8B (popular size)
  - model_id: "amd/Llama-3.1-8B-Instruct-FP8-KV"
    container_name: "vllm-inference"
    input_lengths: [512]
    output_lengths: [512]
    batch_sizes: [1, 8]
    dtype: "float16"
    tensor_parallel_size: 1
    quantization: null
    trust_remote_code: false
    model_params: 8.03e9
    num_layers: 32
    num_heads: 32
    hidden_size: 4096
    head_dim: 128
    kv_cache_dtype: "fp8"
  
  # Qwen3-8B (representative Qwen model)
  - model_id: "Qwen/Qwen3-8B"
    container_name: "vllm-inference"
    input_lengths: [512]
    output_lengths: [512]
    batch_sizes: [1, 8]
    dtype: "float16"
    tensor_parallel_size: 1
    quantization: null
    trust_remote_code: false
    model_params: 8.0e9
    num_layers: 32
    num_heads: 32
    hidden_size: 4096
    head_dim: 128
  
  # Kimi K2 (Moonshot AI) - FIXED with trust_remote_code
  - model_id: "moonshotai/Kimi-K2-Instruct-0905"
    container_name: "vllm-inference"
    input_lengths: [512]
    output_lengths: [512]
    batch_sizes: [1, 4]
    dtype: "bfloat16"
    tensor_parallel_size: 4
    quantization: null
    trust_remote_code: true  # ← FIX: Required for custom modeling code
    model_params: 14.0e9
    num_layers: 40
    num_heads: 40
    hidden_size: 4096
  
  # Mistral-7B (baseline comparison)
  - model_id: "mistralai/Mistral-7B-Instruct-v0.2"
    container_name: "vllm-inference"
    input_lengths: [512]
    output_lengths: [512]
    batch_sizes: [1, 8]
    dtype: "float16"
    tensor_parallel_size: 1
    quantization: null
    trust_remote_code: false
    model_params: 7.24e9
    num_layers: 32
    num_heads: 32
    hidden_size: 4096
    head_dim: 128
  
  # DeepSeek-R1-0528 (reasoning model)
  - model_id: "deepseek-ai/DeepSeek-R1-0528"
    container_name: "vllm-inference"
    input_lengths: [512, 1024]
    output_lengths: [512]
    batch_sizes: [1, 4, 8]
    dtype: "bfloat16"
    tensor_parallel_size: 2
    quantization: null
    trust_remote_code: true
    model_params: 32.0e9
    num_layers: 60
    num_heads: 32
    hidden_size: 4096
    head_dim: 128

# Quick test summary:
# - 6 models
# - 2-3 configs per model
# - Total: ~15 configurations
# - Estimated time: 2-3 hours
# - Mix of sizes: 1B, 7-8B, 14B, 54B (MoE)
# - Tests: FP8-KV, standard FP16, MoE, different architectures
# - ✅ FIXED: Kimi-K2 now has trust_remote_code=true
