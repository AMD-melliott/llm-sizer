models:
- hf_model_id: zai-org/GLM-4.5-Air
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 256
  - 512
  kv_cache_dtype: null
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
  - 1024
- hf_model_id: zai-org/GLM-4.5
  tensor_parallel_size: 8
  batch_sizes:
  - 1
  - 4
  quantization: null
  output_lengths:
  - 256
  - 512
  kv_cache_dtype: null
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
- hf_model_id: zai-org/GLM-4.5-FP8
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 8
  - 16
  quantization: fp8
  output_lengths:
  - 512
  kv_cache_dtype: fp8
  dtype: fp8
  container_name: vllm-inference
  input_lengths:
  - 512
  - 1024
- hf_model_id: zai-org/GLM-4.6
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 256
  kv_cache_dtype: null
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
- hf_model_id: amd/Llama-3.2-1B-Instruct-FP8-KV
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 8
  - 16
  - 32
  quantization: null
  output_lengths:
  - 256
  - 512
  - 1024
  kv_cache_dtype: fp8
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
  - 1024
  - 2048
- hf_model_id: amd/Llama-3.1-8B-Instruct-FP8-KV
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 4
  - 8
  - 16
  quantization: null
  output_lengths:
  - 256
  - 512
  kv_cache_dtype: fp8
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
  - 1024
- hf_model_id: amd/Llama-3.1-70B-Instruct-FP8-KV
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 256
  - 512
  kv_cache_dtype: fp8
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
  - 1024
- hf_model_id: amd/Llama-3.3-70B-Instruct-FP8-KV
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 512
  kv_cache_dtype: fp8
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 512
  - 1024
- hf_model_id: Qwen/Qwen3-4B
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 8
  - 16
  quantization: null
  output_lengths:
  - 256
  - 512
  kv_cache_dtype: null
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
  - 1024
- hf_model_id: Qwen/Qwen3-8B
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 4
  - 8
  - 16
  quantization: null
  output_lengths:
  - 256
  - 512
  kv_cache_dtype: null
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
  - 1024
- hf_model_id: Qwen/Qwen3-14B
  tensor_parallel_size: 2
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 256
  - 512
  kv_cache_dtype: null
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
  - 1024
- hf_model_id: Qwen/Qwen3-30B-A3B-Instruct-2507
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 256
  - 512
  kv_cache_dtype: null
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
- hf_model_id: Qwen/Qwen3-235B-A22B-Instruct-2507
  tensor_parallel_size: 8
  batch_sizes:
  - 1
  - 2
  - 4
  quantization: null
  output_lengths:
  - 256
  kv_cache_dtype: null
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
- hf_model_id: moonshotai/Kimi-K2-Instruct-0905
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 256
  - 512
  kv_cache_dtype: null
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
  - 1024
- hf_model_id: deepseek-ai/DeepSeek-R1-0528
  tensor_parallel_size: 8
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 256
  - 512
  kv_cache_dtype: null
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
  - 1024
- hf_model_id: deepseek-ai/DeepSeek-V3.1
  tensor_parallel_size: 8
  batch_sizes:
  - 1
  - 2
  - 4
  quantization: null
  output_lengths:
  - 256
  kv_cache_dtype: null
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
- hf_model_id: deepseek-ai/DeepSeek-V3.1-Base
  tensor_parallel_size: 8
  batch_sizes:
  - 1
  - 2
  quantization: null
  output_lengths:
  - 256
  kv_cache_dtype: null
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
- hf_model_id: openai/gpt-oss-20b
  tensor_parallel_size: 2
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 256
  - 512
  kv_cache_dtype: null
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
  - 1024
- hf_model_id: openai/gpt-oss-120b
  tensor_parallel_size: 8
  batch_sizes:
  - 1
  - 4
  quantization: null
  output_lengths:
  - 256
  - 512
  kv_cache_dtype: null
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
- hf_model_id: mistralai/Mistral-7B-Instruct-v0.2
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 8
  - 16
  quantization: null
  output_lengths:
  - 256
  - 512
  kv_cache_dtype: null
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
  - 1024
- hf_model_id: mistralai/Mixtral-8x7B-Instruct-v0.1
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 256
  - 512
  kv_cache_dtype: null
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
- hf_model_id: google/gemma-3-12b-it
  tensor_parallel_size: 2
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 256
  - 512
  kv_cache_dtype: null
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
  - 1024
- hf_model_id: google/gemma-3-27b-it
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  quantization: null
  output_lengths:
  - 256
  - 512
  kv_cache_dtype: null
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 256
  - 512
