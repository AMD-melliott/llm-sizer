models:
- hf_model_id: mistralai/Mistral-7B-Instruct-v0.2
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 8
  quantization: null
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 1024
- hf_model_id: TheBloke/Mistral-7B-Instruct-v0.2-AWQ
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 8
  quantization: awq
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 1024
- hf_model_id: TheBloke/Mistral-7B-Instruct-v0.2-GPTQ
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 8
  quantization: gptq
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 1024
- hf_model_id: amd/Llama-3.1-8B-Instruct-FP8-KV
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 8
  quantization: null
  output_lengths:
  - 512
  kv_cache_dtype: fp8
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 1024
- hf_model_id: hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 8
  quantization: awq
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 1024
- hf_model_id: hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 8
  quantization: gptq
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 1024
- hf_model_id: TheBloke/Llama-2-13B-chat-AWQ
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 8
  quantization: awq
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 1024
