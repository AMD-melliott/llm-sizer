models:
- hf_model_id: mistralai/Mixtral-8x7B-Instruct-v0.1
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 1024
- hf_model_id: mistralai/Mixtral-8x7B-Instruct-v0.1
  tensor_parallel_size: 2
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 1024
- hf_model_id: mistralai/Mixtral-8x7B-Instruct-v0.1
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 512
  dtype: float16
  container_name: vllm-inference
  input_lengths:
  - 1024
- hf_model_id: zai-org/GLM-4.5-Air
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 512
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 512
  - 1024
  - 2048
- hf_model_id: Qwen/Qwen2-57B-A14B-Instruct
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 512
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 1024
- hf_model_id: Qwen/Qwen2-57B-A14B-Instruct
  tensor_parallel_size: 8
  batch_sizes:
  - 1
  - 4
  - 8
  quantization: null
  output_lengths:
  - 512
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 1024
- hf_model_id: deepseek-ai/DeepSeek-V2-Lite
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  quantization: null
  output_lengths:
  - 512
  dtype: bfloat16
  container_name: vllm-inference
  input_lengths:
  - 1024
