# Example Batch Profiling Configuration
# 
# This file defines models to profile and their configurations.
# The script will automatically start containers, profile them, and clean up.
#
# Usage:
#   python scripts/batch-profile-models.py --config scripts/configs/models-example.yaml

models:
  # Small model example - quick profiling
  - model_id: "meta-llama/Llama-3.2-3B-Instruct"
    container_name: "llama-3b-profile"
    image: "vllm/vllm-openai:latest"
    batch_sizes: [1, 4, 8]
    max_tokens: [256, 512, 1024]
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.9
    port: 8001
    prompts:
      - "Explain quantum computing"
      - "Write a Python function to sort a list"

  # Medium model with tensor parallelism
  - model_id: "meta-llama/Llama-3.1-8B-Instruct"
    container_name: "llama-8b-profile"
    image: "vllm/vllm-openai:latest"
    batch_sizes: [1, 4, 8, 16]
    max_tokens: [512, 1024, 2048]
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.9
    port: 8002

  # Large model with multi-GPU
  - model_id: "meta-llama/Llama-3.1-70B-Instruct"
    container_name: "llama-70b-profile"
    image: "vllm/vllm-openai:latest"
    batch_sizes: [1, 4, 8]
    max_tokens: [512, 1024]
    tensor_parallel_size: 4
    gpu_memory_utilization: 0.95
    port: 8003
    gpu_ids: "0,1,2,3"  # For AMD GPUs

  # Quantized model example
  - model_id: "meta-llama/Llama-3.1-70B-Instruct"
    container_name: "llama-70b-awq-profile"
    image: "vllm/vllm-openai:latest"
    batch_sizes: [1, 8, 16]
    max_tokens: [1024, 2048]
    tensor_parallel_size: 2
    gpu_memory_utilization: 0.9
    port: 8004
    quantization: "awq"
    gpu_ids: "0,1"

  # Multimodal model example
  - model_id: "llava-hf/llava-1.5-7b-hf"
    container_name: "llava-7b-profile"
    image: "vllm/vllm-openai:latest"
    batch_sizes: [1, 2, 4]
    max_tokens: [512, 1024]
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.9
    port: 8005
