models:
- hf_model_id: meta-llama/Llama-3.2-3B-Instruct
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 4
  - 8
  container_name: llama-3b-profile
  image: vllm/vllm-openai:latest
  max_tokens:
  - 256
  - 512
  - 1024
  gpu_memory_utilization: 0.9
  port: 8001
  prompts:
  - Explain quantum computing
  - Write a Python function to sort a list
- hf_model_id: meta-llama/Llama-3.1-8B-Instruct
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 4
  - 8
  - 16
  container_name: llama-8b-profile
  image: vllm/vllm-openai:latest
  max_tokens:
  - 512
  - 1024
  - 2048
  gpu_memory_utilization: 0.9
  port: 8002
- hf_model_id: meta-llama/Llama-3.1-70B-Instruct
  tensor_parallel_size: 4
  batch_sizes:
  - 1
  - 4
  - 8
  gpu_ids: 0,1,2,3
  container_name: llama-70b-profile
  image: vllm/vllm-openai:latest
  max_tokens:
  - 512
  - 1024
  gpu_memory_utilization: 0.95
  port: 8003
- hf_model_id: meta-llama/Llama-3.1-70B-Instruct
  tensor_parallel_size: 2
  batch_sizes:
  - 1
  - 8
  - 16
  quantization: awq
  gpu_ids: 0,1
  container_name: llama-70b-awq-profile
  image: vllm/vllm-openai:latest
  max_tokens:
  - 1024
  - 2048
  gpu_memory_utilization: 0.9
  port: 8004
- hf_model_id: llava-hf/llava-1.5-7b-hf
  tensor_parallel_size: 1
  batch_sizes:
  - 1
  - 2
  - 4
  container_name: llava-7b-profile
  image: vllm/vllm-openai:latest
  max_tokens:
  - 512
  - 1024
  gpu_memory_utilization: 0.9
  port: 8005
