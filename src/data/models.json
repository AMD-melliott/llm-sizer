{
  "models": [
    {
      "id": "florence-2-base",
      "name": "Florence 2 base",
      "hidden_size": 768,
      "num_layers": 6,
      "num_heads": 12,
      "default_context_length": 1024,
      "architecture": "transformer",
      "modality": "multimodal",
      "vision_config": {
        "model_type": "davit",
        "image_size": 224,
        "patch_size": [
          7,
          3,
          3,
          3
        ],
        "num_channels": 3,
        "hidden_size": 768,
        "num_layers": 12,
        "num_heads": 12,
        "parameters_millions": 86
      },
      "multimodal_config": {
        "image_token_count": 1024,
        "max_images": 1,
        "projector_type": "mlp",
        "projector_params_millions": 23.1,
        "merge_strategy": "concatenate",
        "supports_video": false
      },
      "intermediate_size": 3072,
      "vocab_size": 51289,
      "parameters_billions": 0.231567705
    },
    {
      "id": "gemma-3-1b-it",
      "name": "gemma 3 1b it",
      "hidden_size": 1152,
      "num_layers": 26,
      "num_heads": 4,
      "default_context_length": 32768,
      "architecture": "transformer",
      "num_kv_heads": 1,
      "intermediate_size": 6912,
      "vocab_size": 262144,
      "parameters_billions": 0.999885952
    },
    {
      "id": "llama-3-2-1b-instruct",
      "name": "Llama 3.2 1B Instruct",
      "hidden_size": 2048,
      "num_layers": 16,
      "num_heads": 32,
      "default_context_length": 131072,
      "architecture": "transformer",
      "num_kv_heads": 8,
      "intermediate_size": 8192,
      "vocab_size": 128256,
      "parameters_billions": 1.2358144
    },
    {
      "id": "qwen3-vl-2b-instruct",
      "name": "Qwen3 VL 2B Instruct",
      "hidden_size": 2048,
      "num_layers": 28,
      "num_heads": 16,
      "default_context_length": 262144,
      "architecture": "transformer",
      "modality": "multimodal",
      "vision_config": {
        "model_type": "qwen3_vl",
        "image_size": 224,
        "patch_size": 16,
        "num_channels": 3,
        "hidden_size": 1024,
        "num_layers": 12,
        "num_heads": 12,
        "intermediate_size": 4096,
        "parameters_millions": 152
      },
      "multimodal_config": {
        "image_token_count": 196,
        "max_images": 1,
        "projector_type": "mlp",
        "projector_params_millions": 8.4,
        "merge_strategy": "concatenate",
        "supports_video": false
      },
      "num_kv_heads": 8,
      "intermediate_size": 6144,
      "vocab_size": 151936,
      "parameters_billions": 2.127532032
    },
    {
      "id": "gemma-2-2b-it",
      "name": "gemma 2 2b it",
      "hidden_size": 2304,
      "num_layers": 26,
      "num_heads": 8,
      "default_context_length": 8192,
      "architecture": "transformer",
      "num_kv_heads": 4,
      "intermediate_size": 9216,
      "vocab_size": 256000,
      "parameters_billions": 2.614341888
    },
    {
      "id": "llama-3-2-3b-instruct",
      "name": "Llama 3.2 3B Instruct",
      "hidden_size": 3072,
      "num_layers": 28,
      "num_heads": 24,
      "default_context_length": 131072,
      "architecture": "transformer",
      "num_kv_heads": 8,
      "intermediate_size": 8192,
      "vocab_size": 128256,
      "parameters_billions": 3.212749824
    },
    {
      "id": "phi-3-mini-4k-instruct",
      "name": "Phi 3 mini 4k instruct",
      "hidden_size": 3072,
      "num_layers": 32,
      "num_heads": 32,
      "default_context_length": 4096,
      "architecture": "transformer",
      "num_kv_heads": 32,
      "intermediate_size": 8192,
      "vocab_size": 32064,
      "parameters_billions": 3.821079552
    },
    {
      "id": "qwen3-vl-4b-instruct",
      "name": "Qwen3 VL 4B Instruct",
      "hidden_size": 2560,
      "num_layers": 36,
      "num_heads": 32,
      "default_context_length": 262144,
      "architecture": "transformer",
      "num_kv_heads": 8,
      "intermediate_size": 9728,
      "vocab_size": 151936,
      "parameters_billions": 4
    },
    {
      "id": "phi-3-vision-128k-instruct",
      "name": "Phi 3 vision 128k instruct",
      "hidden_size": 3072,
      "num_layers": 32,
      "num_heads": 32,
      "default_context_length": 131072,
      "architecture": "transformer",
      "modality": "multimodal",
      "vision_config": {
        "model_type": "clip_vision_tower",
        "image_size": 336,
        "patch_size": 14,
        "num_channels": 3,
        "hidden_size": 1024,
        "num_layers": 24,
        "num_heads": 16,
        "intermediate_size": 4096,
        "parameters_millions": 303
      },
      "multimodal_config": {
        "image_token_count": 144,
        "max_images": 1,
        "projector_type": "mlp",
        "projector_params_millions": 15.7,
        "merge_strategy": "concatenate",
        "supports_video": false
      },
      "num_kv_heads": 32,
      "intermediate_size": 8192,
      "vocab_size": 32064,
      "parameters_billions": 4.14662144
    },
    {
      "id": "deepseek-r1-7b",
      "name": "DeepSeek-R1 7B",
      "parameters_billions": 7,
      "hidden_size": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "default_context_length": 32768,
      "architecture": "transformer"
    },
    {
      "id": "mistral-7b",
      "name": "Mistral 7B",
      "parameters_billions": 7,
      "hidden_size": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "default_context_length": 32768,
      "architecture": "transformer"
    },
    {
      "id": "mistral-7b-instruct-v0-3",
      "name": "Mistral 7B Instruct v0.3",
      "hidden_size": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "default_context_length": 32768,
      "architecture": "transformer",
      "num_kv_heads": 8,
      "intermediate_size": 14336,
      "vocab_size": 32768,
      "parameters_billions": 7.248023552
    },
    {
      "id": "qwen2-5-7b-instruct",
      "name": "Qwen2.5 7B Instruct",
      "hidden_size": 3584,
      "num_layers": 28,
      "num_heads": 28,
      "default_context_length": 32768,
      "architecture": "transformer",
      "num_kv_heads": 4,
      "intermediate_size": 18944,
      "vocab_size": 152064,
      "parameters_billions": 7.615616512
    },
    {
      "id": "llama-3-8b",
      "name": "Llama 3 8B",
      "parameters_billions": 8,
      "hidden_size": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "default_context_length": 8192,
      "architecture": "transformer"
    },
    {
      "id": "qwen3-vl-8b-instruct",
      "name": "Qwen3 VL 8B Instruct",
      "hidden_size": 4096,
      "num_layers": 36,
      "num_heads": 32,
      "default_context_length": 262144,
      "architecture": "transformer",
      "num_kv_heads": 8,
      "intermediate_size": 12288,
      "vocab_size": 151936,
      "parameters_billions": 8
    },
    {
      "id": "llama-3-1-8b-instruct",
      "name": "Llama 3.1 8B Instruct",
      "hidden_size": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "default_context_length": 131072,
      "architecture": "transformer",
      "num_kv_heads": 8,
      "intermediate_size": 14336,
      "vocab_size": 128256,
      "parameters_billions": 8.030261248
    },
    {
      "id": "gemma-2-9b-it",
      "name": "gemma 2 9b it",
      "hidden_size": 3584,
      "num_layers": 42,
      "num_heads": 16,
      "default_context_length": 8192,
      "architecture": "transformer",
      "num_kv_heads": 8,
      "intermediate_size": 14336,
      "vocab_size": 256000,
      "parameters_billions": 9.241705984
    },
    {
      "id": "glm-4-1v-9b-thinking",
      "name": "GLM 4.1V 9B Thinking",
      "hidden_size": 4096,
      "num_layers": 40,
      "num_heads": 32,
      "default_context_length": 65536,
      "architecture": "transformer",
      "modality": "multimodal",
      "vision_config": {
        "model_type": "vision_transformer",
        "image_size": 336,
        "patch_size": 14,
        "num_channels": 3,
        "hidden_size": 1536,
        "num_layers": 12,
        "num_heads": 12,
        "intermediate_size": 13696,
        "parameters_millions": 620
      },
      "multimodal_config": {
        "image_token_count": 576,
        "max_images": 1,
        "projector_type": "mlp",
        "projector_params_millions": 29.4,
        "merge_strategy": "concatenate",
        "supports_video": false
      },
      "num_kv_heads": 2,
      "intermediate_size": 13696,
      "vocab_size": 151552,
      "parameters_billions": 10.292777472
    },
    {
      "id": "deepseek-r1-14b",
      "name": "DeepSeek-R1 14B",
      "parameters_billions": 14,
      "hidden_size": 4096,
      "num_layers": 48,
      "num_heads": 32,
      "default_context_length": 32768,
      "architecture": "transformer"
    },
    {
      "id": "phi-4",
      "name": "phi 4",
      "hidden_size": 5120,
      "num_layers": 40,
      "num_heads": 40,
      "default_context_length": 16384,
      "architecture": "transformer",
      "num_kv_heads": 10,
      "intermediate_size": 17920,
      "vocab_size": 100352,
      "parameters_billions": 14.6595072
    },
    {
      "id": "qwen2-5-14b-instruct",
      "name": "Qwen2.5 14B Instruct",
      "hidden_size": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "default_context_length": 32768,
      "architecture": "transformer",
      "num_kv_heads": 8,
      "intermediate_size": 13824,
      "vocab_size": 152064,
      "parameters_billions": 14.770033664
    },
    {
      "id": "kimi-vl-a3b-instruct",
      "name": "Kimi VL A3B Instruct",
      "hidden_size": 2048,
      "num_layers": 27,
      "num_heads": 16,
      "default_context_length": 131072,
      "architecture": "moe",
      "modality": "multimodal",
      "vision_config": {
        "model_type": "moonvit",
        "image_size": 224,
        "patch_size": 14,
        "num_channels": 3,
        "hidden_size": 1152,
        "num_layers": 27,
        "num_heads": 16,
        "intermediate_size": 4304,
        "parameters_millions": 412
      },
      "multimodal_config": {
        "image_token_count": 256,
        "max_images": 1,
        "projector_type": "mlp",
        "projector_params_millions": 8.9,
        "merge_strategy": "concatenate",
        "supports_video": false
      },
      "num_kv_heads": 16,
      "intermediate_size": 11264,
      "vocab_size": 163840,
      "experts_per_token": 6,
      "parameters_billions": 16.407657776
    },
    {
      "id": "kimi-vl-a3b-thinking",
      "name": "Kimi VL A3B Thinking",
      "hidden_size": 2048,
      "num_layers": 27,
      "num_heads": 16,
      "default_context_length": 131072,
      "architecture": "moe",
      "modality": "multimodal",
      "vision_config": {
        "model_type": "moonvit",
        "image_size": 224,
        "patch_size": 14,
        "num_channels": 3,
        "hidden_size": 1152,
        "num_layers": 27,
        "num_heads": 16,
        "intermediate_size": 4304,
        "parameters_millions": 412
      },
      "multimodal_config": {
        "image_token_count": 256,
        "max_images": 1,
        "projector_type": "mlp",
        "projector_params_millions": 8.9,
        "merge_strategy": "concatenate",
        "supports_video": false
      },
      "num_kv_heads": 16,
      "intermediate_size": 11264,
      "vocab_size": 163840,
      "experts_per_token": 6,
      "parameters_billions": 16.407657776
    },
    {
      "id": "gpt-oss-20b",
      "name": "gpt oss 20b",
      "hidden_size": 2880,
      "num_layers": 24,
      "num_heads": 64,
      "default_context_length": 131072,
      "architecture": "moe",
      "modality": "text",
      "num_kv_heads": 8,
      "intermediate_size": 2880,
      "vocab_size": 201088,
      "num_experts": 32,
      "experts_per_token": 4,
      "parameters_billions": 21.511953984
    },
    {
      "id": "mistral-small-instruct-2409",
      "name": "Mistral Small Instruct 2409",
      "hidden_size": 6144,
      "num_layers": 56,
      "num_heads": 48,
      "default_context_length": 32768,
      "architecture": "transformer",
      "num_kv_heads": 8,
      "intermediate_size": 16384,
      "vocab_size": 32768,
      "parameters_billions": 22.247282688
    },
    {
      "id": "gemma-2-27b-it",
      "name": "gemma 2 27b it",
      "hidden_size": 4608,
      "num_layers": 46,
      "num_heads": 32,
      "default_context_length": 8192,
      "architecture": "transformer",
      "num_kv_heads": 16,
      "intermediate_size": 36864,
      "vocab_size": 256000,
      "parameters_billions": 27.22712832
    },
    {
      "id": "qwen3-vl-30b-a3b-instruct",
      "name": "Qwen3 VL 30B A3B Instruct",
      "hidden_size": 2048,
      "num_layers": 48,
      "num_heads": 32,
      "default_context_length": 262144,
      "architecture": "moe",
      "num_kv_heads": 4,
      "intermediate_size": 6144,
      "vocab_size": 151936,
      "num_experts": 128,
      "experts_per_token": 8,
      "parameters_billions": 30
    },
    {
      "id": "deepseek-r1-32b",
      "name": "DeepSeek-R1 32B",
      "parameters_billions": 32,
      "hidden_size": 5120,
      "num_layers": 60,
      "num_heads": 40,
      "default_context_length": 32768,
      "architecture": "transformer"
    },
    {
      "id": "kimi-k2-instruct",
      "name": "Kimi K2 Instruct",
      "hidden_size": 7168,
      "num_layers": 61,
      "num_heads": 64,
      "default_context_length": 131072,
      "architecture": "moe",
      "modality": "text",
      "num_kv_heads": 64,
      "intermediate_size": 18432,
      "vocab_size": 163840,
      "experts_per_token": 8,
      "parameters_billions": 32
    },
    {
      "id": "glm-z1-32b-0414",
      "name": "GLM Z1 32B 0414",
      "hidden_size": 6144,
      "num_layers": 61,
      "num_heads": 48,
      "default_context_length": 32768,
      "architecture": "transformer",
      "modality": "text",
      "num_kv_heads": 2,
      "intermediate_size": 23040,
      "vocab_size": 151552,
      "parameters_billions": 32.566081536
    },
    {
      "id": "glm-4-32b-0414",
      "name": "GLM 4 32B 0414",
      "hidden_size": 6144,
      "num_layers": 61,
      "num_heads": 48,
      "default_context_length": 32768,
      "architecture": "transformer",
      "modality": "text",
      "num_kv_heads": 2,
      "intermediate_size": 23040,
      "vocab_size": 151552,
      "parameters_billions": 32.566081536
    },
    {
      "id": "qwen2-5-32b-instruct",
      "name": "Qwen2.5 32B Instruct",
      "hidden_size": 5120,
      "num_layers": 64,
      "num_heads": 40,
      "default_context_length": 32768,
      "architecture": "transformer",
      "num_kv_heads": 8,
      "intermediate_size": 27648,
      "vocab_size": 152064,
      "parameters_billions": 32.763876352
    },
    {
      "id": "qwen3-vl-32b-instruct",
      "name": "Qwen3 VL 32B Instruct",
      "hidden_size": 5120,
      "num_layers": 64,
      "num_heads": 64,
      "default_context_length": 262144,
      "architecture": "transformer",
      "modality": "multimodal",
      "vision_config": {
        "model_type": "qwen3_vl",
        "image_size": 224,
        "patch_size": 16,
        "num_channels": 3,
        "hidden_size": 1152,
        "num_layers": 12,
        "num_heads": 12,
        "intermediate_size": 4304,
        "parameters_millions": 184
      },
      "multimodal_config": {
        "image_token_count": 196,
        "max_images": 1,
        "projector_type": "mlp",
        "projector_params_millions": 38,
        "merge_strategy": "concatenate",
        "supports_video": false
      },
      "num_kv_heads": 8,
      "intermediate_size": 25600,
      "vocab_size": 151936,
      "parameters_billions": 33.357390064
    },
    {
      "id": "yi-34b",
      "name": "Yi 34B",
      "parameters_billions": 34,
      "hidden_size": 7168,
      "num_layers": 60,
      "num_heads": 56,
      "default_context_length": 200000,
      "architecture": "transformer"
    },
    {
      "id": "mixtral-8x7b",
      "name": "Mixtral 8x7B",
      "parameters_billions": 46.7,
      "hidden_size": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "default_context_length": 32768,
      "architecture": "moe"
    },
    {
      "id": "deepseek-r1-70b",
      "name": "DeepSeek-R1 70B",
      "parameters_billions": 70,
      "hidden_size": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "default_context_length": 32768,
      "architecture": "transformer"
    },
    {
      "id": "llama-3-70b",
      "name": "Llama 3 70B",
      "parameters_billions": 70,
      "hidden_size": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "default_context_length": 8192,
      "architecture": "transformer"
    },
    {
      "id": "qwen-72b",
      "name": "Qwen 72B",
      "parameters_billions": 72,
      "hidden_size": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "default_context_length": 32768,
      "architecture": "transformer"
    },
    {
      "id": "glm-4-5v",
      "name": "GLM 4.5V",
      "hidden_size": 4096,
      "num_layers": 46,
      "num_heads": 96,
      "default_context_length": 65536,
      "architecture": "moe",
      "modality": "multimodal",
      "vision_config": {
        "model_type": "glm4v_moe",
        "image_size": 336,
        "patch_size": 14,
        "num_channels": 3,
        "hidden_size": 1536,
        "num_layers": 12,
        "num_heads": 12,
        "intermediate_size": 10944,
        "parameters_millions": 519
      },
      "multimodal_config": {
        "image_token_count": 576,
        "max_images": 1,
        "projector_type": "mlp",
        "projector_params_millions": 29.4,
        "merge_strategy": "concatenate",
        "supports_video": false
      },
      "num_kv_heads": 8,
      "intermediate_size": 10944,
      "vocab_size": 151552,
      "experts_per_token": 8,
      "parameters_billions": 107.71093312
    },
    {
      "id": "glm-4-5-air",
      "name": "GLM 4.5 Air",
      "hidden_size": 4096,
      "num_layers": 46,
      "num_heads": 96,
      "default_context_length": 131072,
      "architecture": "moe",
      "modality": "text",
      "num_kv_heads": 8,
      "intermediate_size": 10944,
      "vocab_size": 151552,
      "experts_per_token": 8,
      "parameters_billions": 110.468824832
    },
    {
      "id": "gpt-oss-120b",
      "name": "gpt oss 120b",
      "hidden_size": 2880,
      "num_layers": 36,
      "num_heads": 64,
      "default_context_length": 131072,
      "architecture": "moe",
      "modality": "text",
      "num_kv_heads": 8,
      "intermediate_size": 2880,
      "vocab_size": 201088,
      "num_experts": 128,
      "experts_per_token": 4,
      "parameters_billions": 120.412337472
    },
    {
      "id": "mixtral-8x22b",
      "name": "Mixtral 8x22B",
      "parameters_billions": 141,
      "hidden_size": 6144,
      "num_layers": 56,
      "num_heads": 48,
      "default_context_length": 65536,
      "architecture": "moe"
    },
    {
      "id": "gpt-3-175b",
      "name": "GPT-3 175B",
      "parameters_billions": 175,
      "hidden_size": 12288,
      "num_layers": 96,
      "num_heads": 96,
      "default_context_length": 2048,
      "architecture": "transformer"
    },
    {
      "id": "falcon-180b",
      "name": "Falcon 180B",
      "parameters_billions": 180,
      "hidden_size": 14848,
      "num_layers": 80,
      "num_heads": 232,
      "default_context_length": 2048,
      "architecture": "transformer"
    },
    {
      "id": "qwen3-vl-235b-a22b-instruct",
      "name": "Qwen3 VL 235B A22B Instruct",
      "hidden_size": 4096,
      "num_layers": 94,
      "num_heads": 64,
      "default_context_length": 262144,
      "architecture": "moe",
      "num_kv_heads": 4,
      "intermediate_size": 12288,
      "vocab_size": 151936,
      "num_experts": 128,
      "experts_per_token": 8,
      "parameters_billions": 235
    },
    {
      "id": "glm-4-6",
      "name": "GLM 4.6",
      "hidden_size": 5120,
      "num_layers": 92,
      "num_heads": 96,
      "default_context_length": 202752,
      "architecture": "moe",
      "modality": "text",
      "num_kv_heads": 8,
      "intermediate_size": 12288,
      "vocab_size": 151552,
      "experts_per_token": 8,
      "parameters_billions": 356.785898816
    },
    {
      "id": "glm-4-5",
      "name": "GLM 4.5",
      "hidden_size": 5120,
      "num_layers": 92,
      "num_heads": 96,
      "default_context_length": 131072,
      "architecture": "moe",
      "modality": "text",
      "num_kv_heads": 8,
      "intermediate_size": 12288,
      "vocab_size": 151552,
      "experts_per_token": 8,
      "parameters_billions": 358.337791296
    },
    {
      "id": "snowflake-arctic-instruct",
      "name": "snowflake arctic instruct",
      "hidden_size": 7168,
      "num_layers": 35,
      "num_heads": 56,
      "default_context_length": 4096,
      "architecture": "moe",
      "num_kv_heads": 8,
      "intermediate_size": 4864,
      "vocab_size": 32000,
      "num_experts": 128,
      "experts_per_token": 2,
      "parameters_billions": 478.584608768
    },
    {
      "id": "deepseek-v3",
      "name": "DeepSeek V3",
      "hidden_size": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "default_context_length": 163840,
      "architecture": "moe",
      "num_kv_heads": 128,
      "intermediate_size": 18432,
      "vocab_size": 129280,
      "experts_per_token": 8,
      "parameters_billions": 684.531386
    },
    {
      "id": "deepseek-r1",
      "name": "DeepSeek R1",
      "hidden_size": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "default_context_length": 163840,
      "architecture": "moe",
      "num_kv_heads": 128,
      "intermediate_size": 18432,
      "vocab_size": 129280,
      "experts_per_token": 8,
      "parameters_billions": 684.531386
    },
    {
      "id": "custom",
      "name": "Custom Model",
      "parameters_billions": 7,
      "hidden_size": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "default_context_length": 4096,
      "architecture": "transformer"
    }
  ]
}
